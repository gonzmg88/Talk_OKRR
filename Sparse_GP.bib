
@incollection{snelson_sparse_2006,
	title = {Sparse {Gaussian} {Processes} using {Pseudo}-inputs},
	url = {http://papers.nips.cc/paper/2857-sparse-gaussian-processes-using-pseudo-inputs.pdf},
	urldate = {2017-10-17},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 18},
	publisher = {MIT Press},
	author = {Snelson, Edward and Ghahramani, Zoubin},
	editor = {Weiss, Y. and Schölkopf, P. B. and Platt, J. C.},
	year = {2006},
	pages = {1257--1264},
	file = {NIPS Full Text PDF:/home/gonmagar/Zotero/storage/RRQ65APK/Snelson and Ghahramani - 2006 - Sparse Gaussian Processes using Pseudo-inputs.pdf:application/pdf;NIPS Snapshort:/home/gonmagar/Zotero/storage/QDNHFW7W/2857-sparse-gaussian-processes-using-pseudo-inputs.html:text/html}
}

@inproceedings{seeger_fast_2003,
	title = {Fast {Forward} {Selection} to {Speed} {Up} {Sparse} {Gaussian} {Process} {Regression}},
	url = {http://inverseprobability.com/publications/seeger-fast03.html},
	abstract = {We present a method for the sparse greedy approximation of Bayesian Gaussian process regression, featuring a novel heuristic for very fast forward selection. Our method is essentially as fast as an...},
	urldate = {2017-10-17},
	author = {Seeger, Matthias and Williams, Christopher K. I. and Lawrence, Neil D.},
	year = {2003},
	file = {aistats03-final.pdf:/home/gonmagar/Zotero/storage/KBAA38A8/aistats03-final.pdf:application/pdf;Snapshot:/home/gonmagar/Zotero/storage/P5R9TEP5/seeger-fast03.html:text/html}
}

@inproceedings{titsias_variational_2009,
	title = {Variational {Learning} of {Inducing} {Variables} in {Sparse} {Gaussian} {Processes}},
	volume = {5},
	url = {http://jmlr.csail.mit.edu/proceedings/papers/v5/titsias09a/titsias09a.pdf},
	urldate = {2017-10-17},
	publisher = {Journal of Machine Learning Research - Proceedings Track},
	author = {Titsias, Michalis K.},
	year = {2009},
	pages = {567--574},
	file = {Full Text PDF:/home/gonmagar/Zotero/storage/QWXH4K9B/Titsias - 2009 - Variational Learning of Inducing Variables in Spar.pdf:application/pdf;Snapshot:/home/gonmagar/Zotero/storage/Y75IPT4Y/AISTATS09_Titsias.html:text/html}
}

@article{quinonero-candela_unifying_2005,
	title = {A unifying view of sparse approximate {Gaussian} process regression},
	volume = {6},
	url = {http://www.jmlr.org/papers/volume6/quinonero-candela05a/quinonero-candela05a.pdf},
	abstract = {We provide a new unifying view, including all existing proper probabilistic sparse approximations for Gaussian process regression. Our approach relies on expressing the effective prior which the methods are using. This allows new insights to be gained, and highlights the relationship between existing methods. It also allows for a clear theoretically justified ranking of the closeness of the known approximations to the corresponding full GPs. Finally we point directly to designs of new better sparse approximations, combining the best of the existing strategies, within attractive computational constraints.},
	journal = {Journal of Machine Learning Research},
	author = {Quiñonero-Candela, Joaquin and Rasmussen, Carl Edward and Herbrich, Ralf},
	year = {2005},
	pages = {2005},
	file = {Citeseer - Full Text PDF:/home/gonmagar/Zotero/storage/KT5PDBVU/Quiñonero-candela et al. - 2005 - A unifying view of sparse approximate Gaussian pro.pdf:application/pdf;Citeseer - Snapshot:/home/gonmagar/Zotero/storage/XYT6QF8Q/summary.html:text/html}
}

@article{bui_unifying_2017,
	title = {A {Unifying} {Framework} for {Gaussian} {Process} {Pseudo}-{Point} {Approximations} using {Power} {Expectation} {Propagation}},
	url = {http://arxiv.org/abs/1605.07066},
	abstract = {Gaussian processes (GPs) are flexible distributions over functions that enable high-level assumptions about unknown functions to be encoded in a parsimonious, flexible and general way. Although elegant, the application of GPs is limited by computational and analytical intractabilities that arise when data are sufficiently numerous or when employing non-Gaussian models. Consequently, a wealth of GP approximation schemes have been developed over the last 15 years to address these key limitations. Many of these schemes employ a small set of pseudo data points to summarise the actual data. In this paper, we develop a new pseudo-point approximation framework using Power Expectation Propagation (Power EP) that unifies a large number of these pseudo-point approximations. Unlike much of the previous venerable work in this area, the new framework is built on standard methods for approximate inference (variational free-energy, EP and Power EP methods) rather than employing approximations to the probabilistic generative model itself. In this way, all of approximation is performed at `inference time' rather than at `modelling time' resolving awkward philosophical and empirical questions that trouble previous approaches. Crucially, we demonstrate that the new framework includes new pseudo-point approximation methods that outperform current approaches on regression and classification tasks.},
	urldate = {2017-10-17},
	journal = {arXiv:1605.07066 [cs, stat]},
	author = {Bui, Thang D. and Yan, Josiah and Turner, Richard E.},
	month = oct,
	year = {2017},
	note = {arXiv: 1605.07066},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv\:1605.07066 PDF:/home/gonmagar/Zotero/storage/XR8LHDR8/Bui et al. - 2016 - A Unifying Framework for Gaussian Process Pseudo-P.pdf:application/pdf;arXiv.org Snapshot:/home/gonmagar/Zotero/storage/B9FSLMTY/1605.html:text/html}
}

@incollection{williams_using_2000,
	title = {Using the {Nyström} {Method} to {Speed} {Up} {Kernel} {Machines}},
	url = {http://papers.nips.cc/paper/1866-using-the-nystrom-method-to-speed-up-kernel-machines.pdf},
	urldate = {2017-10-17},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 13},
	publisher = {MIT Press},
	author = {Williams, Christopher K. I. and Seeger, Matthias},
	editor = {Leen, T. K. and Dietterich, T. G. and Tresp, V.},
	year = {2000},
	pages = {682--688},
	file = {NIPS Full Text PDF:/home/gonmagar/Zotero/storage/I7QAJERJ/Williams and Seeger - 2001 - Using the Nyström Method to Speed Up Kernel Machin.pdf:application/pdf;NIPS Snapshort:/home/gonmagar/Zotero/storage/N9R8YCE3/1866-using-the-nystrom-method-to-speed-up-kernel-machines.html:text/html}
}

@article{lazaro-gredilla_sparse_2010,
	title = {Sparse {Spectrum} {Gaussian} {Process} {Regression}},
	volume = {11},
	issn = {1532-4435},
	url = {http://www.jmlr.org/papers/volume11/lazaro-gredilla10a/lazaro-gredilla10a.pdf},
	abstract = {We present a new sparse Gaussian Process (GP) model for regression. The key novel idea is to sparsify the spectral representation of the GP. This leads to a simple, practical algorithm for regression tasks. We compare the achievable trade-offs between predictive accuracy and computational requirements, and show that these are typically superior to existing state-of-the-art sparse approximations. We discuss both the weight space and function space representations, and note that the new construction implies priors over functions which are always stationary, and can approximate any covariance function in this class.},
	urldate = {2017-10-18},
	journal = {The Journal of Machine Learning Research},
	author = {Lázaro-Gredilla, Miguel and Quiñonero-Candela, Joaquin and Rasmussen, Carl Edward and Figueiras-Vidal, Aníbal R.},
	month = aug,
	year = {2010},
	pages = {1865--1881},
	file = {ACM Full Text PDF:/home/gonmagar/Zotero/storage/6LN79HQ8/Lázaro-Gredilla et al. - 2010 - Sparse Spectrum Gaussian Process Regression.pdf:application/pdf}
}

@incollection{cao_efficient_2013,
	title = {Efficient {Optimization} for {Sparse} {Gaussian} {Process} {Regression}},
	url = {http://papers.nips.cc/paper/5087-efficient-optimization-for-sparse-gaussian-process-regression.pdf},
	urldate = {2017-10-19},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 26},
	publisher = {Curran Associates, Inc.},
	author = {Cao, Yanshuai and Brubaker, Marcus A and Fleet, David J and Hertzmann, Aaron},
	editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
	year = {2013},
	pages = {1097--1105},
	file = {NIPS Full Text PDF:/home/gonmagar/Zotero/storage/TR6WKMJM/Cao et al. - 2013 - Efficient Optimization for Sparse Gaussian Process.pdf:application/pdf;NIPS Snapshort:/home/gonmagar/Zotero/storage/9YTMVWZM/5087-efficient-optimization-for-sparse-gaussian-process-regression.html:text/html}
}

@incollection{bauer_understanding_2016,
	title = {Understanding {Probabilistic} {Sparse} {Gaussian} {Process} {Approximations}},
	url = {https://papers.nips.cc/paper/6477-understanding-probabilistic-sparse-gaussian-process-approximations},
	urldate = {2017-10-19},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 29},
	publisher = {Curran Associates, Inc.},
	author = {Bauer, Matthias and van der Wilk, Mark and Rasmussen, Carl Edward},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	year = {2016},
	pages = {1533--1541},
	file = {NIPS Full Text PDF:/home/gonmagar/Zotero/storage/9BGKSCAD/Bauer et al. - 2016 - Understanding Probabilistic Sparse Gaussian Proces.pdf:application/pdf;NIPS Snapshort:/home/gonmagar/Zotero/storage/45VA3DL7/6477-understanding-probabilistic-sparse-gaussian-process-approximations.html:text/html}
}

@article{csato_sparse_2002,
	title = {Sparse {On}-{Line} {Gaussian} {Processes}},
	volume = {14},
	issn = {0899-7667},
	url = {http://www.mitpressjournals.org/doi/10.1162/089976602317250933},
	doi = {10.1162/089976602317250933},
	abstract = {We develop an approach for sparse representations of gaussian process (GP) models (which are Bayesian types of kernel machines) in order to overcome their limitations for large data sets. The method is based on a combination of a Bayesian on-line algorithm, together with a sequential construction of a relevant subsample of the data that fully specifies the prediction of the GP model. By using an appealing parameterization and projection techniques in a reproducing kernel Hilbert space, recursions for the effective parameters and a sparse gaussian approximation of the posterior process are obtained. This allows for both a propagation of predictions and Bayesian error measures. The significance and robustness of our approach are demonstrated on a variety of experiments.},
	number = {3},
	urldate = {2017-10-20},
	journal = {Neural Computation},
	author = {Csató, Lehel and Opper, Manfred},
	month = mar,
	year = {2002},
	pages = {641--668},
	file = {Snapshot:/home/gonmagar/Zotero/storage/B39MJ23Q/089976602317250933.html:text/html}
}

@article{matthews_sparse_2015,
	title = {On {Sparse} variational methods and the {Kullback}-{Leibler} divergence between stochastic processes},
	url = {http://arxiv.org/abs/1504.07027},
	abstract = {The variational framework for learning inducing variables (Titsias, 2009a) has had a large impact on the Gaussian process literature. The framework may be interpreted as minimizing a rigorously defined Kullback-Leibler divergence between the approximating and posterior processes. To our knowledge this connection has thus far gone unremarked in the literature. In this paper we give a substantial generalization of the literature on this topic. We give a new proof of the result for infinite index sets which allows inducing points that are not data points and likelihoods that depend on all function values. We then discuss augmented index sets and show that, contrary to previous works, marginal consistency of augmentation is not enough to guarantee consistency of variational inference with the original model. We then characterize an extra condition where such a guarantee is obtainable. Finally we show how our framework sheds light on interdomain sparse approximations and sparse approximations for Cox processes.},
	urldate = {2017-10-23},
	journal = {19th International Conference on Artificial Intelligence and Statistics},
	author = {Matthews, Alexander G. de G. and Hensman, James and Turner, Richard E. and Ghahramani, Zoubin},
	month = apr,
	year = {2015},
	note = {arXiv: 1504.07027},
	keywords = {Statistics - Machine Learning},
	pages = {231--239},
	file = {arXiv\:1504.07027 PDF:/home/gonmagar/Zotero/storage/7QGVFMN7/Matthews et al. - 2015 - On Sparse variational methods and the Kullback-Lei.pdf:application/pdf;arXiv.org Snapshot:/home/gonmagar/Zotero/storage/BX7ZDUM3/1504.html:text/html}
}

@article{hensman_gaussian_2013,
	title = {Gaussian {Processes} for {Big} {Data}},
	url = {http://arxiv.org/abs/1309.6835},
	abstract = {We introduce stochastic variational inference for Gaussian process models. This enables the application of Gaussian process (GP) models to data sets containing millions of data points. We show how GPs can be vari- ationally decomposed to depend on a set of globally relevant inducing variables which factorize the model in the necessary manner to perform variational inference. Our ap- proach is readily extended to models with non-Gaussian likelihoods and latent variable models based around Gaussian processes. We demonstrate the approach on a simple toy problem and two real world data sets.},
	urldate = {2017-10-23},
	journal = {Twenty-Ninth Conference on Uncertainty in Artificial Intelligence},
	author = {Hensman, James and Fusi, Nicolo and Lawrence, Neil D.},
	month = sep,
	year = {2013},
	note = {arXiv: 1309.6835},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	pages = {282--290},
	file = {arXiv\:1309.6835 PDF:/home/gonmagar/Zotero/storage/62FQMCEP/Hensman et al. - 2013 - Gaussian Processes for Big Data.pdf:application/pdf;arXiv.org Snapshot:/home/gonmagar/Zotero/storage/8E4KVDGB/1309.html:text/html}
}

@inproceedings{gal_improving_2015,
	title = {Improving the {Gaussian} {Process} {Sparse} {Spectrum} {Approximation} by {Representing} {Uncertainty} in {Frequency} {Inputs}},
	url = {http://proceedings.mlr.press/v37/galb15.html},
	abstract = {Standard sparse pseudo-input approximations to the Gaussian process (GP) cannot handle complex functions well. Sparse spectrum alternatives attempt to answer this but are known to over-fit. We sugg...},
	language = {en},
	urldate = {2017-10-26},
	booktitle = {{PMLR}},
	author = {Gal, Yarin and Turner, Richard},
	month = jun,
	year = {2015},
	pages = {655--664},
	file = {Full Text PDF:/home/gonmagar/Zotero/storage/IJV6UU7R/Gal and Turner - 2015 - Improving the Gaussian Process Sparse Spectrum App.pdf:application/pdf;Snapshot:/home/gonmagar/Zotero/storage/MFJG6VR8/galb15.html:text/html}
}